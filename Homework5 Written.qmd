---
title: "Homework 5 Julia Fish"
format: pdf
editor: visual
---

## Task 1: Conceptual Questions

- Question 1: **What is the purpose of using cross-validation when fitting a random forest model?**  
  Cross-validation helps estimate a modelâ€™s performance with "new" data. When fitting a random forest model, cross-validation ensures that the model is not overfitting by evaluating its performance across multiple data splits (i.e. leaving one fold out at a time). Because of this, it gives a more reliable estimate of prediction error than a single train-test split.

- Question 2: **Describe the bagged tree algorithm.**  
  The baged tree algorithm involves generating multiple training sets using bootstrapping techniques. A decision tree is trained on each of the bootstrap resamples, and predictions are averaged over. This reduces variance and increases model stability compared to fitting a single tree model.

- Question 3: **What is meant by a general linear model?**  
  A general linear model is a linear model that can have different forms of response variable than just all real numbers. For example, binomial, poisson, gamma, etc. can be used in general linear models to help modify the response range as well as oher logistical factors in the model fit.

- Question 4: **When fitting a multiple linear regression model, what does adding an interaction term do?**  
  Adding an interaction term to a MLR model allows the effect of one variable on the response to depend on the level of another variable. That is, the variables are allowed to communicate with one another to predict a response value instead of only having just additive effects.

- Question 5: **Why do we split our data into a training and test set?**  
  Using a training and test set allows us to see model performance on "unseen" data with known response values. That way we can get a better idea of whether or not our model performs well or has just been overfit to the data it was given to train the model.
  

## Task 2: Data Prep

### Packages and Data

First, we will load in the packages we need for this task. We will also read in our `heart` data set as a tibble.

```{r message=FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(yardstick)

heart <- as_tibble(read_csv("https://www4.stat.ncsu.edu/~online/datasets/heart.csv"))
```

### Question 1

Now, we will run and discuss the `summary()` of this tibble.

```{r}
summary(heart)
```

Above, we see the full summary of this data set. The `HeartDisease` variable seems to be a quantitative variable, which does not make sense in this case since it is supposed to represent the presence or absence of heart disease. That should be categorical.

### Question 2

Now, we will create a categorical`HeartDisease` variable named `HD`. In the same pipeline, we will also remove `ST_Slope` and the original `HeartDisease` variable. Lastly, we will save this new version of the data under `new_heart`. We will do this below:

```{r}
new_heart <- heart |>
  mutate(HD = factor(HeartDisease)) |>
  select(-ST_Slope, -HeartDisease)
```

## Task 3: EDA

### Question 1

Here, we hope to make a scatterplot to visualize age vs. a function of heart disease and their max heart rate. We will do this by making the x-axis contain max heart rate, the y-axis containing age, and the points being colored by whether or not the person has heart disease with a colorblind friendly pallet. Then, the trend lines will be put on the plot as well.


```{r}
library(ggplot2)
library(scales)

ggplot(new_heart, aes(x = MaxHR, y = Age, color = HD)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_color_brewer(palette = "Dark2") +
  labs(
    title = "Age vs. Max Heart Rate by Heart Disease Presence",
    x = "Maximum Heart Rate (MaxHR)",
    y = "Age",
    color = "Heart Disease"
  ) +
  theme_minimal()
```

### Question 2

Based on this plot, I think an interaction plot would be better than an additive model. That is because the lines are not parallel (or almost parallel). That means that the slope changes with the heart disease status, which points to an interaction model being a more appropriate fit.


## Task 4: Testing and Training

Now, we will split our `new_heart` into an 80-20 spli for training and testing. This will be done below (with a seed of 101 set):

```{r}
set.seed(101)

heart_split <- initial_split(new_heart, prop = 0.8)

train <- training(heart_split)
test <- testing(heart_split)
```


## Task 5: OLS and LASSO

### Question 1

First, we will fit an OLS interaction model named `ols_mlr` as described in the homework. We will then report a summary.

```{r}
ols_mlr <- lm(Age ~ MaxHR * HD, data = train)
summary(ols_mlr)
```

From this summary, we see that the overall model is significant with an F value of 54.84 and an associated p-value of 2.2e-16. In addition, at the 0.05 significance level, all individual predictors are significant as well.

### Question 2

Now, to get a better evaluation of the model's performance, we will predict on the test set and calculate the RMSE using this model below:

```{r}
preds <- predict(ols_mlr, newdata = test)


results <- test %>%
  mutate(pred = preds)

ols_rmse <- rmse(results, truth = Age, estimate = pred)
ols_rmse
```

The RMSE using this model and the test set is 9.100206.


### Question 3

Now, we are going to fit a LASSO model with cross validation to compare to the OLS model fit above. We will start with our recipe as guided in the file:

```{r}

LASSO_recipe <- recipe(Age ~ MaxHR + HD, data = train) |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_numeric_predictors()) |>
  step_interact(~ MaxHR:starts_with("HD_"))

LASSO_recipe
```

### Question 4

Now that out recipe is made, we will set up our `spec` and grid. After that, we will use those to pick a final model and report using `tidy()`. We will do that below:

```{r}
LASSO_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")

set.seed(101)
heart_cv_folds <- vfold_cv(train, 10)


LASSO_wkf <- workflow() |>
  add_recipe(LASSO_recipe) |>
  add_model(LASSO_spec)

LASSO_grid <- LASSO_wkf |>
  tune_grid(resamples = heart_cv_folds,
            grid = grid_regular(penalty(), levels = 200))

LASSO_grid |>
  collect_metrics() |>
  filter(.metric == "rmse")

lowest_rmse <- LASSO_grid |>
  select_best(metric = "rmse")

LASSO_wkf |>
  finalize_workflow(lowest_rmse)

LASSO_final <- LASSO_wkf |>
  finalize_workflow(lowest_rmse) |>
  fit(train)

tidy(LASSO_final)
```

As seen above, we selected the final model.

### Question 5

Without looking at the RMSE values, I would expect them to be lower for the LASSO model due to shrinkage of the coefficients to 0. It can be seen in the estimates column above that the coefficient estimates are much closer to 0 that for the OLS model, which leads me to believe that the variance will be lower and the RMSE will be lower as a result.

### Question 6

Now, we will calculate the RMSE for the LASSO model and compare it to the OLS model, ultimately looking for very similar values.

```{r}
preds2 <- predict(LASSO_final, new_data = test)


LASSO_results <- test %>%
  mutate(pred = preds2$.pred)

LASSO_rmse <- rmse(LASSO_results, truth = Age, estimate = pred)
LASSO_rmse

```

This value is very similar to the RMSE value obtained for the OLS model (9.100206).

### Question 7

The RMSE calculations ended up being roughly the same even though the coefficients are different since the shrinkage is targeted toward taking out unnecessary information in the model. In addition, the shrinkage affects ALL predictor coefficients in the model, which means that the prediction can stay fairly precise even if all of the coefficients differ. Said differently, the variance decreases with the LASSO model, but the bias increases, so that trade off makes the predictions still able to be similar to the OLS model.


